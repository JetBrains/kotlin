{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# WASM Benchmark Comparison\n",
    "\n",
    "This notebook allows you to compare the performance of WASM benchmarks across two different git revisions.\n"
   ],
   "id": "ebd7005d48f0b39b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install dependencies if they are missing\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    print(f\"Installing {package}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "try:\n",
    "    import matplotlib\n",
    "except ImportError:\n",
    "    install_package(\"matplotlib\")\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    install_package(\"numpy\")\n"
   ],
   "id": "e862c76805bb19d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def run_command(command, cwd=None):\n",
    "    print(f\"Executing: {command}\")\n",
    "    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, cwd=cwd)\n",
    "    output = []\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "        output.append(line)\n",
    "    process.wait()\n",
    "    if process.returncode != 0:\n",
    "        raise Exception(f\"Command failed with exit code {process.returncode}\")\n",
    "    return \"\".join(output)\n"
   ],
   "id": "9b21d6663312a512",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_benchmark_results(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        return []\n",
    "    \n",
    "    entries = []\n",
    "    with open(file_path, 'r', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if not row.get('Median'):\n",
    "                continue\n",
    "            entry = {\n",
    "                'test': os.path.basename(row['Test'].strip()),\n",
    "                'mode': row['Mode'],\n",
    "                'median': int(row['Median'])\n",
    "            }\n",
    "            entries.append(entry)\n",
    "            \n",
    "    return entries\n"
   ],
   "id": "f4c0d159ff543235",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_benchmarks_for_revision(revision, test_filter=\"org.jetbrains.kotlin.wasm.test.FirWasmSpecCodegenBenchmarkTestGenerated\"):\n",
    "    # 0. Save current branch/commit\n",
    "    original_rev = subprocess.check_output(\"git rev-parse --abbrev-ref HEAD\", shell=True, text=True).strip()\n",
    "    if original_rev == \"HEAD\":\n",
    "        original_rev = subprocess.check_output(\"git rev-parse HEAD\", shell=True, text=True).strip()\n",
    "        \n",
    "    # 1. Stash current changes\n",
    "    # run_command(\"git stash\")\n",
    "    \n",
    "    try:\n",
    "        # 2. Checkout revision\n",
    "        run_command(f\"git checkout {revision}\")\n",
    "        \n",
    "        # 3. Clear old results file if it exists\n",
    "        results_file = \"benchmark_results.csv\"\n",
    "        if os.path.exists(results_file):\n",
    "            os.remove(results_file)\n",
    "            \n",
    "        # 4. Run benchmarks\n",
    "        # Note: we use --no-daemon to avoid issues with different compiler versions in the same daemon\n",
    "        try:\n",
    "            run_command(f\"./gradlew :wasm:wasm.tests:test --tests {test_filter} --no-daemon\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Benchmark execution failed for revision {revision}.\")\n",
    "            print(f\"Error: {e}\")\n",
    "            print(f\"Continuing to parse whatever results were produced.\")\n",
    "        \n",
    "        # 5. Parse and return results\n",
    "        return parse_benchmark_results(results_file)\n",
    "        \n",
    "    finally:\n",
    "        # Restore original state (best effort)\n",
    "        run_command(f\"git checkout {original_rev}\")\n"
   ],
   "id": "5f57da472a5e77c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Comparison Execution\n",
    "\n",
    "Specify the two revisions you want to compare below. They can be commit hashes, branch names, or tags.\n"
   ],
   "id": "5977a858d69b7311"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "REVISION_A = \"vsirotkina/benchmark-no-ss\"\n",
    "REVISION_B = \"vsirotkina/benchmark-with-ss\"\n",
    "TEST_FILTER = \"org.jetbrains.kotlin.wasm.test.FirWasmSpecCodegenBenchmarkTestGenerated\"\n",
    "\n",
    "print(f\"Starting benchmark for Revision A: {REVISION_A}\")\n",
    "results_a = run_benchmarks_for_revision(REVISION_A, TEST_FILTER)\n",
    "\n",
    "print(f\"\\nStarting benchmark for Revision B: {REVISION_B}\")\n",
    "results_b = run_benchmarks_for_revision(REVISION_B, TEST_FILTER)\n"
   ],
   "id": "bc5149dc2fe73014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_comparison(results_a, results_b, label_a=\"Revision A\", label_b=\"Revision B\"):\n",
    "    # Organize data by (test, mode)\n",
    "    data = {}\n",
    "    \n",
    "    for r in results_a:\n",
    "        key = (r['test'], r['mode'])\n",
    "        if key not in data: data[key] = {}\n",
    "        data[key]['a'] = r['median']\n",
    "        \n",
    "    for r in results_b:\n",
    "        key = (r['test'], r['mode'])\n",
    "        if key not in data: data[key] = {}\n",
    "        data[key]['b'] = r['median']\n",
    "        \n",
    "    # Filter keys that have both results\n",
    "    common_keys = [k for k in data if 'a' in data[k] and 'b' in data[k]]\n",
    "    common_keys.sort()\n",
    "    \n",
    "    if not common_keys:\n",
    "        print(\"No common results found to compare.\")\n",
    "        return\n",
    "        \n",
    "    labels = [f\"{k[0]} ({k[1]})\" for k in common_keys]\n",
    "    vals_a = [data[k]['a'] / 1_000_000 for k in common_keys]  # Convert to ms\n",
    "    vals_b = [data[k]['b'] / 1_000_000 for k in common_keys]  # Convert to ms\n",
    "    \n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    rects1 = ax.bar(x - width/2, vals_a, width, label=label_a)\n",
    "    rects2 = ax.bar(x + width/2, vals_b, width, label=label_b)\n",
    "    \n",
    "    ax.set_ylabel('Median Time (ms)')\n",
    "    ax.set_title('Benchmark Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(f\"{ 'Test (Mode)':<50} | {label_a:>12} | {label_b:>12} | { 'Diff (%)':>10}\")\n",
    "    print(\"-\" * 95)\n",
    "    for i, k in enumerate(common_keys):\n",
    "        a, b = data[k]['a'], data[k]['b']\n",
    "        diff = (b - a) / a * 100\n",
    "        print(f\"{labels[i]:<50} | {vals_a[i]:12.2f} | {vals_b[i]:12.2f} | {diff:+10.2f}%\")\n",
    "\n",
    "if 'results_a' in locals() and 'results_b' in locals():\n",
    "    plot_comparison(results_a, results_b, REVISION_A, REVISION_B)\n"
   ],
   "id": "f7c6104ac159268e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
